# -*- coding: utf-8 -*-
"""Final_project_PR.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BthPPxXVK1RM2Fk0H1GuE6lFEY911x9R
"""

import nltk
from nltk.translate.bleu_score import corpus_bleu

!pip install --upgrade googletrans==4.0.0-rc1
!pip install wget
!pip install kaggle

from transformers import GPT2TokenizerFast, ViTImageProcessor, VisionEncoderDecoderModel
from torch.utils.data import Dataset
from transformers import GPT2TokenizerFast, ViTImageProcessor, VisionEncoderDecoderModel
from nltk.translate.bleu_score import corpus_bleu, sentence_bleu
from googletrans import Translator
from PIL import Image
import numpy as np
import pandas as pd
import wget
import requests
import torch
import numpy as np
from PIL import Image
import pickle
from googletrans import Translator
import matplotlib.pyplot as plt
from nltk.translate.bleu_score import sentence_bleu
from nltk.translate.bleu_score import corpus_bleu
import json
import os
from tqdm import tqdm
import pandas as pd
import warnings
import csv
warnings.filterwarnings('ignore')

from google.colab import drive
drive.mount('/content/drive')

os.environ['KAGGLE_CONFIG_DIR'] = '/content/drive/MyDrive/kaggle'

!kaggle datasets download -d saifsust/bancap
!unzip bancap.zip

!kaggle datasets download -d adityajn105/flickr8k
!unzip flickr8k.zip

data = pd.read_csv("BAN-Cap_captiondata.csv")
data.head()

data['caption_id']=data['caption_id'].str[:-2]
data.head()

model_raw = VisionEncoderDecoderModel.from_pretrained("nlpconnect/vit-gpt2-image-captioning")

# Image Processor and token for the model
image_processor = ViTImageProcessor.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
tokenizer       = GPT2TokenizerFast.from_pretrained("nlpconnect/vit-gpt2-image-captioning")

def show_n_generate(url, greedy = True, model = model_raw):
    #image = Image.open(requests.get(url, stream =True).raw)
    image = Image.open(url)
    pixel_values   = image_processor(image, return_tensors ="pt").pixel_values
    plt.imshow(np.asarray(image))
    plt.show()

    if greedy:
        generated_ids  = model.generate(pixel_values, max_new_tokens = 30)
    else:
        generated_ids  = model.generate(
            pixel_values,
            do_sample=True,
            max_new_tokens = 30,
            top_k=5)
    generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]
    return generated_text

def translate_to_bangla(text):
  translator = Translator()
  translated = translator.translate(text, src = 'en', dest = 'bn')
  return translated.text

generated_english_captions = []
translated_sentences = []
reference_captions = []

for i, image in enumerate(data['caption_id']):
    if i < 500:
        if i % 5 == 0:
            image_file = "/".join(["Images", image])
            generated_english_caption = show_n_generate(image_file, greedy=False, model=model_raw)
            print("English caption: ", generated_english_caption)
            translated_sentence = translate_to_bangla(generated_english_caption)
            print("Generated Bangla caption:", translated_sentence)
            reference_caption = data['bengali_caption'][i]
            print("Reference Bangla caption:", reference_caption)

            generated_english_captions.append(generated_english_caption)
            translated_sentences.append(translated_sentence)
            reference_captions.append(reference_caption)

# Define the file path for the CSV file
csv_file_path = "final_500.csv"

# Open the CSV file in write mode
with open(csv_file_path, 'w', newline='', encoding='utf-8') as csvfile:
    # Create a CSV writer object
    writer = csv.writer(csvfile)

    # Write the header row
    writer.writerow(["Generated English Caption", "Translated Bengali Sentence", "Reference Bengali Caption"])

    # Iterate over the lists and write each row to the CSV file
    for eng_caption, translated_sentence, ref_caption in zip(generated_english_captions, translated_sentences, reference_captions):
        writer.writerow([eng_caption, translated_sentence, ref_caption])

print("CSV file has been created successfully.")

final_results = pd.read_csv('final_500.csv')
final_results.head()

nltk.download('punkt')

references = [[ref for sublist in ref_list for ref in sublist] for ref_list in reference_captions]
predictions = [[pred for sublist in pred_list for pred in sublist] for pred_list in translated_sentences]

bleu_score = corpus_bleu(references, predictions)
print("BLEU Score:", bleu_score)

"""Since this approach kept yielding insignificant results, we chose another approach as shown in the next cell for evaluation."""

# Load the data
data = pd.read_csv("BAN-Cap_captiondata.csv")
data['caption_id'] = data['caption_id'].str[:-2]

# Load the image processor and tokenizer
image_processor = ViTImageProcessor.from_pretrained("nlpconnect/vit-gpt2-image-captioning")
tokenizer = GPT2TokenizerFast.from_pretrained("nlpconnect/vit-gpt2-image-captioning")

# Function to generate captions and translate to Bengali
def generate_and_translate(image_path, model, reference_caption):
    image = Image.open(image_path)
    pixel_values = image_processor(image, return_tensors="pt").pixel_values

    # Generate caption
    generated_ids = model.generate(pixel_values, max_length=30, num_return_sequences=1)
    generated_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]

    # Translate to Bengali
    translator = Translator()
    translated_sentence = translator.translate(generated_text, src='en', dest='bn').text

    return translated_sentence, reference_caption

# Load pre-trained model
model = VisionEncoderDecoderModel.from_pretrained("nlpconnect/vit-gpt2-image-captioning")

# Prepare data for evaluation
evaluation_data = data.head(50)  # Considering the first 50 samples for evaluation

# Initialize lists for predictions and references
predictions = []
references = []

# Generate and translate captions for evaluation
for index, row in evaluation_data.iterrows():
    image_file = "Images/" + row['caption_id']
    translated_caption, reference_caption = generate_and_translate(image_file, model, row['bengali_caption'])
    predictions.append(translated_caption)
    references.append(reference_caption)

# Convert reference captions to the required format
reference_caption_list = [[ref] for ref in references]

# Calculate BLEU score
bleu_score = corpus_bleu(reference_caption_list, predictions)
print("BLEU Score:", bleu_score)

# Optional: Calculate BLEU score for each individual sentence
bleu_scores_sentence = [sentence_bleu(ref, pred) for ref, pred in zip(reference_caption_list, predictions)]
print("Individual BLEU Scores:", bleu_scores_sentence)
